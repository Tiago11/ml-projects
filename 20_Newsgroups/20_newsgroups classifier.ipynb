{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to find the topic of a document. We'll use a set of documents, included in **sklearn**, known as **20 Newsgroups**. It's a set of approximately 18,000 emails, distributed between 20 distinct topics such as **christian religion**, **atheism**, **guns**, **mac hardware**, **graphic computing**, etc. The set has topics that are related to each other and has other that are not. \n",
    "\n",
    "We are going to create a classifier for a subset of the topics. This subset will have a total of 5 topics. 3 related topics and 3 unrelated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing the necessary modules. Load the training data from the **20 Newsgroup** dataset.  \n",
    "\n",
    "After that we select the topics we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# This are the selected topics.\n",
    "cats = ['rec.sport.baseball', 'rec.sport.hockey', 'rec.autos', 'sci.crypt', 'soc.religion.christian']\n",
    "\n",
    "# Load the training data for the chosen categories.\n",
    "newsgroup_train = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42)\n",
    "\n",
    "# List the categories of the fetched data.\n",
    "list(newsgroup_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 ¿Cuántos archivos tiene su dataset?\n",
    "Imprima, del 4to archivo, el tema y las 30 primeras líneas.\n",
    "Imprima las categorías de los 1eros 20 documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see:\n",
    "* How many emails do we have in our dataset.\n",
    "* See how a email looks like.\n",
    "* The categories of the first 20 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails in dataset ---> (2985,)\n",
      "Topic of the 4th email ---> rec.sport.baseball\n",
      "First 30 lines of the 4th email ---> \n",
      "\n",
      "\n",
      "I'm not quite sure how these numbers are generated.  It appears that in\n",
      "a neutral park Bo's HR and slugging tend to drop (he actually loses two\n",
      "home runs).  Or do they?  What is \"equivalent average?\"\n",
      "\n",
      "One thing, when looking at Bo's stats, is that you can see that KC took\n",
      "away some homers.  Normally, you expect some would-be homers to go for\n",
      "doubles or triples in big parks, or to be caught, and for that matter you\n",
      "expect lots of doubles and triples anyway.  But Bo, despite his speed, \n",
      "hit very few doubles and not that many triples.  So I would expect his\n",
      "value to have risen quite considerably in a neutral park.  \n",
      "\n",
      "\n",
      "Felix Jose has been a .350/.440 player in a fairly neutral park.\n",
      "I would offhand guess the `89-`90 Bo at around a .330/.530 player.\n",
      "Maybe .330/.550 .  Not even close.\n",
      "\n",
      "\n",
      "I'd put him about there too.  \n",
      "\n",
      "Note: I hadn't realized the media had hyped him so much.  I thought he\n",
      "was always viewed by them as a better football player, and only so-so \n",
      "at baseball.  He did only have one 30-hr, 100-rbi season, and KC wasn't\n",
      "winning.\n",
      "\n",
      "Note 2: I maybe have harped on this a bit in the past, but there is a\n",
      "mistake being made (by the SDCN's, as they are known, on this group)\n",
      "with respect to players like Bo and Deion and Lofton (and perhaps others).\n",
      "\n",
      "Topics of the first 20 emails --->\n",
      "\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "rec.autos\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "sci.crypt\n",
      "rec.autos\n",
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# Get the amount of emails in the dataset.\n",
    "size = newsgroup_train.filenames.shape\n",
    "print('Emails in dataset ---> ' + str(size))\n",
    "\n",
    "# Get the topic of the 4th email.\n",
    "tema_index = newsgroup_train.target[3]\n",
    "tema = newsgroup_train.target_names[tema_index]\n",
    "print('Topic of the 4th email ---> ' + str(tema))\n",
    "\n",
    "# Get the first 30 lines of the 4th email.\n",
    "primeras_lineas = newsgroup_train.data[3].split('\\n')[0:29]\n",
    "print('First 30 lines of the 4th email ---> ')\n",
    "for linea in primeras_lineas:\n",
    "    print linea\n",
    "print\n",
    "    \n",
    "# Get the topics of the first 20 emails.\n",
    "primeras_cat_ind = newsgroup_train.target[0:19]\n",
    "print('Topics of the first 20 emails --->')\n",
    "print\n",
    "for categoria in primeras_cat_ind:\n",
    "    print newsgroup_train.target_names[categoria]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emails contain metadata, this metadata is mainly emails addresses, universities, linked articles. Keeping this information will cause the classifier to overfit the training data. If we want our classifier to be useful with text outside of this context, we must avoid using the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a new training set, this one without metadata.\n",
    "newsgroup_train = fetch_20newsgroups(subset='train', categories=cats, shuffle=True, random_state=42,\n",
    "                                     remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to vectorize the emails using two different vectorizers. By vectorizing we create a bag-of-words, where each email becomes a \"bag\" of words where the grammar and order of the words doesn't matter. The only thing that matter is the multiplicity of the words (after striping the stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary ---> 30975 words.\n",
      "Maximum amount of words in an email ---> 10176 words.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "vectors = vectorizer.fit_transform(newsgroup_train.data)\n",
    "\n",
    "print('Size of the vocabulary ---> ' + str(len(vectorizer.vocabulary_)) + ' words.')\n",
    "\n",
    "c_vectorizer = CountVectorizer()\n",
    "count_vectors = c_vectorizer.fit_transform(newsgroup_train.data)\n",
    "\n",
    "tam = np.max(count_vectors.sum(axis=1))\n",
    "print('Maximum amount of words in an email ---> ' + str(tam) + ' words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a MultinomialNB classifier and check it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report de Multinomial Naive Bayes --->\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             rec.autos       0.95      0.87      0.91       396\n",
      "    rec.sport.baseball       0.95      0.84      0.89       397\n",
      "      rec.sport.hockey       0.79      0.94      0.86       399\n",
      "             sci.crypt       0.93      0.91      0.92       396\n",
      "soc.religion.christian       0.91      0.94      0.93       398\n",
      "\n",
      "           avg / total       0.91      0.90      0.90      1986\n",
      "\n",
      "Confusion Matrix --->\n",
      "[[344   6  27   9  10]\n",
      " [  5 335  36  10  11]\n",
      " [  4   5 376   6   8]\n",
      " [  6   5  19 360   6]\n",
      " [  2   2  15   3 376]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Get the test dataset and vectorize it\n",
    "newsgroup_test = fetch_20newsgroups(subset='test', categories=cats, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "vectors_test = vectorizer.transform(newsgroup_test.data)\n",
    "\n",
    "# Train the classifier.\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroup_train.target)\n",
    "\n",
    "# Predict the value of the test dataset.\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "# Get some performance metrics.\n",
    "print('Classification Report de Multinomial Naive Bayes --->')\n",
    "print(metrics.classification_report(newsgroup_test.target, pred, target_names=newsgroup_test.target_names))\n",
    "print('Confusion Matrix --->')\n",
    "print(metrics.confusion_matrix(newsgroup_test.target, pred))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a SGDClassifier and check it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report de SGDClassifier --->\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             rec.autos       0.80      0.93      0.86       396\n",
      "    rec.sport.baseball       0.91      0.84      0.87       397\n",
      "      rec.sport.hockey       0.93      0.91      0.92       399\n",
      "             sci.crypt       0.95      0.86      0.90       396\n",
      "soc.religion.christian       0.92      0.94      0.93       398\n",
      "\n",
      "           avg / total       0.90      0.90      0.90      1986\n",
      "\n",
      "Confusion Matrix --->\n",
      "[[370   7   4   9   6]\n",
      " [ 27 334  23   5   8]\n",
      " [ 15  13 362   1   8]\n",
      " [ 35   9   1 339  12]\n",
      " [ 17   4   1   3 373]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a new classifier.\n",
    "clf2 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "clf2.fit(vectors, newsgroup_train.target)\n",
    "\n",
    "# Predict the result of the test dataset.\n",
    "pred2 = clf2.predict(vectors_test)\n",
    "\n",
    "# Get some performance metrics.\n",
    "print('Classification Report de SGDClassifier --->')\n",
    "print(metrics.classification_report(newsgroup_test.target, pred2, target_names=newsgroup_test.target_names))\n",
    "print('Confusion Matrix --->')\n",
    "print(metrics.confusion_matrix(newsgroup_test.target, pred2))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a new approach, we are going to collapse the topics that are related into a new category. After that we will re-train the classifiers and find their new performance ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report de Multinomial Naives Bayes --->\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             sci.crypt       0.96      0.89      0.92       396\n",
      "soc.religion.christian       0.94      0.94      0.94       398\n",
      "                   rec       0.95      0.97      0.96      1192\n",
      "\n",
      "           avg / total       0.95      0.95      0.95      1986\n",
      "\n",
      "Confusion Matrix --->\n",
      "[[ 354    5   37]\n",
      " [   3  374   21]\n",
      " [  13   20 1159]]\n",
      "\n",
      "Classification report de SGDClassifier --->\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "             sci.crypt       1.00      0.62      0.77       396\n",
      "soc.religion.christian       0.99      0.76      0.86       398\n",
      "                   rec       0.83      1.00      0.91      1192\n",
      "\n",
      "           avg / total       0.89      0.88      0.87      1986\n",
      "\n",
      "Confusion Matrix --->\n",
      "[[ 247    1  148]\n",
      " [   0  303   95]\n",
      " [   1    3 1188]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define two sets of categories. One with the unrelated topics and the other with the related ones.\n",
    "sub_cats1 = ['sci.crypt', 'soc.religion.christian']\n",
    "sub_cats2 = ['rec.sport.baseball', 'rec.sport.hockey', 'rec.autos']\n",
    "\n",
    "# Get the training data for the set of categories that are unrelated.\n",
    "sub_data1_train = fetch_20newsgroups(subset='train', categories=sub_cats1, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "\n",
    "# Get the training data for the set of categories that are related.\n",
    "sub_data2_train = fetch_20newsgroups(subset='train', categories=sub_cats2, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "\n",
    "# Collapse the data into one category with name 'rec'.\n",
    "sub_data2_train.target = np.ones(len(sub_data2_train.target))*2\n",
    "sub_data2_train.target_names = ['rec']\n",
    "\n",
    "# Merge the two datasets.\n",
    "sub_data1_train.target_names.extend(sub_data2_train.target_names)\n",
    "sub_data1_train.target = np.append(sub_data1_train.target, sub_data2_train.target)\n",
    "sub_data1_train.filenames = np.append(sub_data1_train.filenames, sub_data2_train.filenames)\n",
    "sub_data1_train.data.extend(sub_data2_train.data)\n",
    "\n",
    "# Repeat the above procedure, this time with the test data.\n",
    "sub_data1_test = fetch_20newsgroups(subset='test', categories=sub_cats1, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "\n",
    "sub_data2_test = fetch_20newsgroups(subset='test', categories=sub_cats2, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "\n",
    "sub_data2_test.target = np.ones(len(sub_data2_test.target))*2\n",
    "sub_data2_test.target_names = ['rec']\n",
    "\n",
    "sub_data1_test.target_names.extend(sub_data2_test.target_names)\n",
    "sub_data1_test.target = np.append(sub_data1_test.target, sub_data2_test.target)\n",
    "sub_data1_test.filenames = np.append(sub_data1_test.filenames, sub_data2_test.filenames)\n",
    "sub_data1_test.data.extend(sub_data2_test.data)\n",
    "\n",
    "# Create a vectorizer and vectorize the data.\n",
    "vectorizer2 = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "vectors2 = vectorizer2.fit_transform(sub_data1_train.data)\n",
    "vectors_test2 = vectorizer.transform(sub_data1_test.data)\n",
    "\n",
    "# Create a classifier and train it.\n",
    "clf3 = MultinomialNB(alpha=.01)\n",
    "clf3.fit(vectors2, sub_data1_train.target)\n",
    "\n",
    "# Predict the results of the test data.\n",
    "pred3 = clf3.predict(vectors_test2)\n",
    "\n",
    "# Print the performance.\n",
    "print('Classification report de Multinomial Naives Bayes --->')\n",
    "print(metrics.classification_report(sub_data1_test.target, pred3, target_names=sub_data1_test.target_names))\n",
    "print('Confusion Matrix --->')\n",
    "print(metrics.confusion_matrix(sub_data1_test.target, pred3))\n",
    "print('')\n",
    "\n",
    "# Create the second classifier and train it.\n",
    "clf4 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "clf4.fit(vectors2, sub_data1_train.target)\n",
    "\n",
    "# Predict the results of the test data.\n",
    "pred4 = clf4.predict(vectors_test2)\n",
    "\n",
    "# Print the performance.\n",
    "print('Classification report de SGDClassifier --->')\n",
    "print(metrics.classification_report(sub_data1_test.target, pred4, target_names=sub_data1_test.target_names))\n",
    "print('Confusion Matrix --->')\n",
    "print(metrics.confusion_matrix(sub_data1_test.target, pred4))\n",
    "print('')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
